<html lang="en-us"><head>
    <meta charset="UTF-8">
    <title> Low-Resource Adaptation for Personalized Co-Speech Gesture Generation </title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700" rel="stylesheet" type="text/css">
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <link href="assets/css/style2.css" rel="stylesheet" type="text/css">
    <link href="assets/css/style.css" rel="stylesheet" type="text/css">
</head>
<body style="background-color: white;">
    <section class="page-header">
        <h1 class="project-name" style="font-size: 2.4rem"> Continual Learning for <br>Personalized Co-Speech Gesture Generation</h1>
        <h2 class="project-tagline"><a href="http://chahuja.com">Chaitanya Ahuja</a>, <a href="https://pratikmjoshi.github.io/">Pratik Joshi</a>, <a href="https://sites.google.com/view/ryoishii/">Ryo Ishii</a>, <a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency</a></h2>
        <h2 class="project-tagline">ICCV 2023</h2>
        <a href="https://bit.ly/cdiffgan" class="button">Paper</a>
        <!-- <a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Ahuja_Low-Resource_Adaptation_for_CVPR_2022_supplemental.pdf" class="button">Supplementary</a> -->
        <a href="" class="button">Code (coming soon)</a>
        <a href="http://chahuja.com/pats" class="button">Dataset Website</a>
        <!-- <a href="https://github.com/chahuja/pats" class="button">Dataset Scripts</a> -->
        <!-- <a href="https://github.com/chahuja/mix-stage" class="btn">View on GitHub</a>
            <a href="https://github.com/chahuja/mix-stage/zipball/master" class="btn">Download .zip</a>
            <a href="https://github.com/chahuja/mix-stage/tarball/master" class="btn">Download .tar.gz</a> -->
        </section>

        <section id="main-content" class="main-content" style="max-width: 75rem; min-width: 75rem; font-size: 0.9rem; background-color: white;">
            <h1>Abstract</h1>
            Co-speech gestures are a key channel of human communication, making them important for personalized chat agents to generate. In the past, gesture generation models assumed that data for each speaker is available all at once, and in large amounts. However in practical scenarios, speaker data comes sequentially and in small amounts as the agent personalizes with more speakers, akin to a continual learning paradigm. While more recent works have shown progress in adapting to low-resource data, they catastrophically forget the gesture styles of initial speakers they were trained on. Also, prior generative continual learning works are not multimodal, making this space less studied. In this paper, we explore this new paradigm and propose C-DiffGAN: an approach that continually learns new speaker gesture styles with only a few minutes of per-speaker data, while retaining previously learnt styles. Inspired by prior continual learning works, C-DiffGAN encourages knowledge retention by 1) generating reminiscences of previous low-resource speaker data, then 2) crossmodally aligning to them to mitigate catastrophic forgetting. We quantitatively demonstrate improved performance and reduced forgetting over strong baselines through standard continual learning measures, reinforced by a qualitative user study that shows that our method produces more natural, style-preserving gestures. Code and videos can be found at https://chahuja.com/cdiffgan

            <h1>Video</h1>
            <center>
                <iframe width="720" height="360" src="videos/iccv2023-video-cdiffgan.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
            </center>


            <h1> Acknowledgements </h1>
            <p align="left">
                This material is based upon work partially supported by the National Science Foundation (Awards #1750439 #1722822), National Institutes of Health, NTT Japan and the InMind project. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of National Science Foundation or National Institutes of Health, and no official endorsement should be inferred.
            </p>

            <hr width="940" style="border-bottom: 2px solid black;">
            <footer class="site-footer">
                <span class="site-footer-owner"><a href="https://github.com/chahuja/cdiffgan"> Low-Resource Adaptation for Personalized Co-Speech Gesture Generation</a> is maintained by <a href="https://github.com/chahuja">chahuja</a>.</span>
                <span class="site-footer-credits">This page is a modified version of the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> created by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
            </footer>

            <iframe id="speakers" class="hidden">




                <meta charset="utf-8">
                <meta name="google" value="notranslate">
